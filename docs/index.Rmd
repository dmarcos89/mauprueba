---
title: "Homework - Multivariate Analysis"
author: "Javier Esteban Aragoneses - Mauricio Marcos Fajgenbaun"
date: "12/12/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# An analysis of Diabetes in Indian Women

## Motivation of Research and Dataset 

For this research, we selected a data set of the Diabetes. Diabetes is an illness that occurs when the sugar in blood (also called glucose) is too high. At the same time, insulin (produced by the pancreas), is produced to help the glucose make it to human cells. When the pancreas does not produce enough insulin or none at all, then this glucose remains in the blood causing serious health issues.

With the years, diabetes had become a larger problem, especially for western societies. Eating fast food or high-processed meals have led to enormous ingestions of sugar and, with this, an historical increase of diabetes in most countries of the world. 

This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The main objective that explains the reason of the existence of this data set, is to diagnostically predict weather or not a patient has diabetes, based on certain measurements of several variables that are included in the dataset. At the same time, this database contains only data of women of at least 21 years old of Pima Indian heritage (north American indigenous women, that live in the State of Arizona and the Mexican states of Sonora and Chihuahua).  Diabetes is quite prevalent in this group of Native Americans living in those places in particular. 

So, we can say that the aim is to study the cause of diabetes in this particular ethnic group. This disease was very uncommon in Pima Indians until the second half of the 20th Century. Nevertheless, as King et al. informs in 1993, the highest prevalence of type 2 diabetes in the world was found in this particular ethnic group, as more than half of the women older than 35 years-old would suffer from this disease. 

So, in this research we will study how health predictors are associated with the presence of diabetes in Pima Indians. According to the World Health Organization criteria, if the 2-hour post-load plasma glucose was at least 200 mg/dl at any survey examination or if found during any moutine medical care. 

In this dataset, 768 women were registered in the database. From this total, a 35% (268 in total) had diabetes, while the rest (500: 65%) did not suffer from this disease. 
In the next table, we show the variables included in the data set with they specific classification. 

##  Variables Description

Pregnancy: quantitative - discrete
Glucose: quantitative - continuous
Blood Pressure: quantitative - continuous
Skin Thickness: quantitative - continuous
Insulin quantitative - continuous
BMI: quantitative - continuous
Diabetes Pedigree Function: quantitative - continuous
Age: quantitative - discrete
Outcome: qualitative- nominal

The variable “Outcome” is a categorical variable that accounts for weather a specific person is diabetic (takes value = 1) or not diabetic (takes value = 0). 

## Exploratory Analysis

First, let´s install all the packages we will use. Then, we will read the file and start working on it.

```{r}
library("tidyverse")
library("ggplot2")
library("dplyr")
library("moments")
library("gridExtra")
library("rrcov")
library("rpart.plot")
library("mice")
library("MASS")
library("andrews")
library("ggcorrplot")
library("FactoMineR")
library("factoextra")
library("paran")
library("corrplot")
```

```{r}
ColClasses=c(rep("numeric",9))
DATOS=read.csv2("diabetes.csv",sep = ",",header = T, colClasses = ColClasses, dec = "." )
summary(DATOS)
```


When studying the variables, we can tell that there are some missing values in some of the variables. This will be a big burden later, when trying to study correlation between the variables and the real dimension of it. Also, it will blur our understanding of the data. Thus, we will impute the missing values first.

To do so, we change the values "0" from the second to the eight variable to "NA". We only make use of these 7 variables, because the first variable (pregnancy) can take the value "0" and our variable number 9 is "outcome" and its value is either "1" or "0". Of course, we wouldn´t want to change their zero values. Nevertheless, we know that as we are talking about humans (that are actually alive) it is impossible to get an observation of these variables equal to zero. Easy example: it is obvious nobody has a blood preassure equal to zero. This is why we consider values "0" for these variables as missing values.

We will impute the values with the library "MICE". 
```{r}
data <- DATOS[2:8]
data[data==0] <-  NA
DATOS[2:8] <- data
summary(DATOS[,1:8])
```
```{r}
sum(is.na(DATOS))
```

This is a big problem. We can see that we have a lot of missing value in our data (652 values are missing). 
When checking more closely, we can say that at least 30% of our rows are being affected by this problem. We certainly can not give up 30% of our data, as we would lose too much information. We have to impute this values using the package "Mice".

```{r}
DATOS <- mice(DATOS,m=1,method="pmm")
DATOS <- complete(DATOS)
attach(DATOS)
```
Now that we solved the problem of missing values, we will create a new variable called "Dataset.norm" to use later on, when we will need to standarize our variables for principal component analysis.
```{r}
Dataset.norm <- DATOS
# Unitization with zero minimum ((x-min)/range))
Dataset.norm[1:8] <- as.data.frame(lapply(DATOS[1:8], normalize))
```

Let´s first dig in into our categorical variable. 

```{r}
ggplot(data=DATOS, aes(x = Outcome)) + geom_bar(aes(fill = Outcome)) + ggtitle("Bar Plot por Outcome")
frec_cal <- table(DATOS$Outcome)
frec_cal
tabl_cont <- prop.table(table(DATOS$Outcome))
tabl_cont
```

As we can see, 65% of our sample does not suffer from diabetes, while 35% does suffer from diabetes. This is a huge proportion, given that this sample is taken from one population in particular (it is certanily a population with a huge incidence of diabetes). One every three persons in this sample has diabetes.

Let´s check the behaviour of our quantitative variables by themselves first. Now, we will check their histograms.
```{r}
hist(Age)
hist(Pregnancies)
hist(Glucose)
hist(BloodPressure)
hist(SkinThickness)
hist(Insulin)
hist(BMI)
hist(DiabetesPedigreeFunction)
```
And then, each boxplot.

```{r}
par(mfrow=c(1,1))
boxplot(Age,main ="Age")
boxplot(Pregnancies, main="Pregnancies")
boxplot(Glucose,main="Glucose")
boxplot(BloodPressure,main="Blood Pressure")
boxplot(SkinThickness, main = "Skin Thickness")
boxplot(Insulin, main = "Insulin")
boxplot(BMI, main = "BMI")
boxplot(DiabetesPedigreeFunction, main="Diabetes Pedigree Function")
```

Let´s now plot the kernel for each variable by itself, and then each kernel by subgroups: diabetic and non-diabetic.
```{r}
dat <- DATOS
dat$Outcome <- as.factor(dat$Outcome)
```

```{r}
univar_graph <- function(univar_name, univar, DATOS, output_var) {
  g_1 <- ggplot(DATOS, aes(x=univar)) + geom_density() + xlab(univar_name)
  g_2 <- ggplot(DATOS, aes(x=univar, fill=output_var)) + geom_density(alpha=0.4) + xlab(univar_name)
  grid.arrange(g_1, g_2, ncol=2, top=paste(univar_name,"variable", "/ [ Skew:",skewness(univar),"]"))
}

for (x in 1:(ncol(DATOS)-1)) {
  univar_graph(names(DATOS)[x], DATOS[,x], DATOS, dat[,'Outcome'])
}

```


```{r}
par(mfrow=c(1,1))

boxplot(Age~Outcome,ylab="Outcome",xlab="Age",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(Pregnancies~Outcome,ylab="Outcome",xlab="Pregnancies",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(Glucose~Outcome,ylab="Outcome",xlab="Glucose",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(BloodPressure~Outcome,ylab="Outcome",xlab="BloodPresure",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(SkinThickness~Outcome,ylab="Outcome",xlab="SkinThickness",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(Insulin~Outcome,ylab="Outcome",xlab="Insulin",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(BMI~Outcome,ylab="Outcome",xlab="BMI",col=c("lightblue","orange"),horizontal=TRUE)
boxplot(DiabetesPedigreeFunction~Outcome,ylab="Outcome",xlab="Diabetes Green function",col=c("lightblue","orange"),horizontal=TRUE)
```
Let´s comment on the plots done above.

### Age
This variable account for the age of each women in the sample. It varies from a minimum of 21 to a maximum of 81. This means that were included women in that age frame. As we can see in the plot below, the median is on 29, meaning that half of the women are more that 29 years old and the other half is older than 29. As the first quartile is on 24 and the third is on 41, half of the women from this sample is between those ages (24 and 41). This shows us that the population is relatively young, although there are 25% of women in the sample that are from 41 to 81 years old. 

At the same time, we can see that when comparing by outcome (if they resulted ill or not) the median of the diabetic ones is higher than the non-diabetic ones. The distribution of this variable looks more left-skewed for the non-diabetic women, in comparison with the ones that are diabetic. This, all together, may mean that inside the non-diabetic woman, most of them are pretty young, and some of them are old. The same happens with the diabetic women: half of them are younger than 35, and the rest are older. 

### Blood Preassure
When studying blood pressure, we must first say that it is measured in mm/gg. The median is around 70, and the distribution looks a bit symmetrical. When dividing the observations between diabetic and not diabetic, we can say that both distributions look pretty similar.  

### BMI
The body mass index is a very useful index to take into account the weight, but related to the height of the person. This way, it is measured: weight (in Kg) / [height (in meters)] ^2. 
As we can see both in the histogram and in the boxplot, the distribution seems symmetrical and the size of the box is quite small (so half of the women of the sample, between the first and third quantile, are concentrated in the values 27.30 and 36.60. Normally having a value between 25 and 30 is considered right, while more than 30 is considered overweight, and more than 35 obesity. 


### The  Diabetes Pedigree Function
The diabetes Pedigree Function scores the likelihood of diabetes based on family history. So the larger value this variable takes, the more likely to be diabetic. As we can see in the plots, our distribution in this case seems right skewed. 
At the same time, when comparing between groups (diabetic and not diabetic) the distributions look  similar, although the third quartile for the diabetic group goes further than the not diabetic one. At the same time, the 4th quartile looks bigger in the case of the not diabetic group


### Glucose
Glucose measures the plasma glucose concentration over 2 hours in an oral glucose tolerance test. As it can be seen, the median is 117 and is very close to the mean (120.9) meaning that it is a bit symmetrical. At the same time, there is some skewedness, as the tail on the right looks much heavier. When classifying between diabetic or not, the median of level of plasma glucose in women with diabetes is higher than the ones that have no diabetes. At the same time, the diabetic women present a wider range of glucose values between the 1st. and 3rd. quartile. 


### Insulin
Insulin is measured in mu U/ml and its distribution looks right-skewed. When looking at the histograms, most of the values are situated in the left side of the distribution, meaning that most women of the sample have low insulin level. A “standard” or “normal” level for this 2 hour test is considered at around 16 to 166 U/ml and the median of our variable in the sample is 30.5 (not very high). When dividing it between diabetic or not, both distributions look similar (with the same long tail) and the biggest interval of frequency at the lowest level of the distribution.  


### Pregnancies
This variable account for the number of pregnancies that each women had during their life. The minimum in the sample is 0 pregnancies and the maximum is 17 pregnancies. The distribution has a heavier tail on the left, meaning it is right-skewed. This is coherent with the fact that most of the woman from the sample are relatively young, and thus did not have so many kids yet. This explains the fact that the median is at 3, meaning that half of the women have less than 3 kids while the rest account for more than 3. Then, the third quartile is at 6, meaning that only 25% of the women fron the sample have between 6 and 17 kids. 
When studying this variable by group of diabetic and not diabetic, we can say that the median of the diabetic women is higher than the one of not diabetic.  


### Skin Thickness
The skin thickness measures the triceps skin fold thickness in mm. As it can be seen in the plots, the distribution looks pretty right skewed, meaning that most of the observation occur in the smallest values of the variable. The median is at 23, meaning 50% of the women have less than 23mm of skin thickness while the other half have more. At the same time, when dividing between groups of diabetic or not, both distribution look similar, although the diabetic looks less dispersed and its median is larger than the ones of non-diabetics. 

In general, there are some variables that are highly positively skewed (Insulin, DiabetesPedigreeFunction, Age) while others are highly negative skewed like BloodPressure.

### Multivariate Analysis

We now perform a matrix of plots, showing possible correlation between different variables (every variable against every variable). We can not say we see a clear pattern of linear correlation between variables. At least, not yet.

```{r}
X_quan <- DATOS[,1:8]
pairs(X_quan,pch=20,col="lightblue")
colors_Student <- c("lightblue","orange")[1*(Outcome=="1")+1]
pairs(X_quan,pch=19,col=colors_Student)
```

Then, we perform a PCP. The Parallel Coordinates Plot is very useful to visualize and analyze high-dimensional data. 

```{r}
parcoord(X_quan,col="lightblue",var.label=TRUE,main="PCP for diabetics")
parcoord(X_quan,col=colors_Student,var.label=TRUE,main="PCP for diabetes in terms of Outcome")
```

As we might expect, being diabetic is directly related to glucose level. There is also a lesser correlation between the other variables and diabetes, except in cases of pregnancy and diabetes pedigree function in which there is no relationship. 

```{r}
par(mfrow=c(1,1))
andrews(as.data.frame(cbind(X_quan,as.factor(DATOS[,9]))),clr=8,ymax=4,main="Andrews' Plot for women in terms of Diabetic or not")
```

# Outliers Detection

In order to study the mean vector, covariance and correlation matrix, we need to take into account outliers, as these measures tend not to be robust. This means that the presence of outliers may change some of our conclusions. This is why we will do this treatment now.

In order to be able to use MCD to treat outliers, we must do some preprocessing of our data.
This is because we need our variables to behave as Gaussian variables (symmetric at least). To achieve this, we perform logarithmic transformations when required. Let´s plot histograms for each variable and take a look at each of their behaviour. It is important to check for outlyers because they may affect our interpretatin of the data.

After looking at the histograms, we can tell that variables: "age", "pregnancies", "skinthickness", "insulin" need an appropiate transformation. 

```{r}
# "Ages" transformation
Ages= log(Age)
hist(Ages)
DATOS$Age <- Ages

# We standarize the variable Pregnancies
hist(DATOS$Pregnancies)
DATOS$Pregnancies=scale(Pregnancies, center = TRUE,scale= TRUE)
hist(DATOS$Pregnancies)

```

```{r}
# We normalize
DATOS$SkinThickness <- scale(log(DATOS$SkinThickness))

# We normalize
hist(log(Insulin))
DATOS$Insulin <- log(DATOS$Insulin)
attach(DATOS)
```


Now that we finished with our transformations, we proceed on applying the MCD.
```{r}
X=DATOS[1:8]
n <- nrow(X)
p <- ncol(X)
color_1 <- "deepskyblue2"
color_2 <- "seagreen2"
color_3 <- "orange2"
MCD_est <- CovMcd(X,alpha=0.75,nsamp="deterministic")
m_MCD <- MCD_est$center
```
Let´s now check on the mean vector, covariance and correlation matrix of the variables before taking the outliers out.


Let´s check first the mean vector, correlation and covariance matrix before taking out outliers.
```{r}
m <- colMeans(DATOS[,1:8])
m
S <- cov(X)
S
R <- cor(X)
R
ggcorrplot(R)
```

## Outliers Treatment

We will find the rows with observations that can be considered outliers.
```{r}
#Let´s check on the outliers

X_sq_Mah_MCD <- MCD_est$mah

col_outliers_Mah_MCD <- rep(color_2,n)
outliers_Mah_MCD <- which(X_sq_Mah_MCD>qchisq(.99,p))
outliers_Mah_MCD
```

 We can plot and check for these outliers aboev the Mahalanobis Distance. 
```{r}
col_outliers_Mah_MCD[outliers_Mah_MCD] <- color_3
par(mfrow=c(1,2))
plot(1:n,X_sq_Mah_MCD,pch=19,col=col_outliers_Mah_MCD,main="Squared Mahalanobis distances",xlab="Observation",ylab="Squared Mahalanobis distance")
abline(h=qchisq(.99,p),lwd=3,col="red")
plot(1:n,log(X_sq_Mah_MCD),pch=19,col=col_outliers_Mah_MCD,main="Log of squared Mahalanobis distances",xlab="Observation",ylab="Log of squared Mahalanobis distance")
abline(h=log(qchisq(.99,p)),lwd=3,col="red")
```
Now, we see the observations bellow and above the log of squared Mahalanobis distance. This is a very good way of observing our observations that fall very far away from the centroid of the data. We can also see the outliers in a scatterplot.


We can plot a Scatterplot showing the outliers. 
```{r}
pairs(X,pch=19,col=col_outliers_Mah_MCD)


attach(DATOS)
```

It is easy to see that there are some observations 
Now, we can substract these rows of observations, in order to be able to understand the mean, and correlations with a better view.

```{r}
C=DATOS[-c(5  , 9  ,13  ,14  ,19  ,40,  44,  46,  59,  68 ,107 ,121 ,126 ,130 ,160 ,178 ,188,194,222, 229, 246 ,255 ,295 ,304 ,324 ,331 ,371 ,372 ,396 ,409,435,446 ,454, 457, 465 ,488,520 ,538 , 550, 580 ,594 ,597, 598 ,622 ,623 ,662 ,667 ,674 ,692 ,703 ,745),]

DATOS <- C
```
Even taking out the outliers, these following variables don´t look so gaussian
```{r}
hist(C$Pregnancies)
hist(C$Age)
```
As we have two subgroups inside of our data, people with diabetes and people not suffering from this dessease (and our objective is obviously related with understanding how this illness behaves among this specific population) we will perform this study for both subgroups and for the total, and then compare them.
```{r}
cd <- C[ C$Outcome==1,]
cd <- cd[,1:8]
cnd <- C[ C$Outcome==0,]
cnd <- cnd[,1:8]
attach(DATOS)
```

First, let´s study this three things for the entire group. 

```{r}
# For the sample totallity
# Mean Vector
m_MCD
```

```{r}
# Covariance Matrix
S_MCD <- MCD_est$cov
S_MCD
```

```{r}
# Correlation Matrix
R_MCD <- cov2cor(S_MCD)
R_MCD
```
Let´s plot this correlation matrix.

```{r}
ggcorrplot(R_MCD)

```
As we can see in this plot, there are some interesting linear relationships between some variables. First, let us say there is no negative linear relationship between any variable. We only have positive correlations, as the negative ones are very close to zero.

They are detailed as follows:

a) there is a positive linear relationship that may be significant between "age" and "pregnancy" meaning that the older people gets they have more pregnancies. This is very trivial: the chance of having had more kids obviosuly increases when the person is older, as pregnancies only increase with age, or stay the same (it is the total amount of pregnancies, a cummulative quantity).
b) Both "insulin" and "BMI" are positively correlated with "glucose". This does not surprise us either. People with higher body mass index have more chances of having high levels of sugar in blood. At the same time, people with high insulin levels after applying the medical test and check their insulin response, tend to have higher levels of glucose.
c) "BMI" and "SkinThickness" also appear to have a significant linear relationship between them. This may tell us thay people with higher body mass index tend to have higher values for skinthikness (and the other way around). This is also reasonable.
d) "Age" looks positively correlated to every other variable measured, with exception of the pedigree diabetes function. The more aged the person, the higher their values of insulin, blood preassure, skinthickness, glucose, BMI. 

Let´s check what happens when we include the variable "Output" in our correlation matrix.

```{r}
Y <- DATOS [,9]
Mat_cov <- cov(DATOS)
mat_cor <- cov2cor(Mat_cov)
mat_cor
```
As we can see, the only value that calls our attention is the correlation between "Outcome" and "Glucose", for being far away from 0. If the correlation between a categorical and quantitative variable is close to 1, it means that subjects with outcome = 1 (diabetics) have larger values than subjects with outcome = 0 (non-diabetic). This basically says that in general the diabetic subset of the sample will have a larger value of glucose in blood. Not a big surprise.


Now, first for the diabetic subgroup (outcome == 1).

```{r}
# Mean Vector
apply(cd,2,mean)
# Covariance Matrix
cov(cd)
l=cov(cd)
# Correlation Matrix
cov2cor(l)
```
Same procedure, for the non-diabetic subgroup (outcome == 0).

```{r}
# Mean Vector
apply(cnd,2,mean)
```

```{r}
# Covariance Matrix
cov(cnd)
n=cov(cnd)
```

```{r}
# Correlation Matrix
cov2cor(n)
```

Women with diabetes seem to have more pregnancies, have in average higher glucose, blood preassure, skin thickness, BMI. It also seems that there is a positive correlation between age and diabetes (older people tend to get it more) and the variance of the glucose is much larger in non-diabetic people.

# Let´s plot a scatterplot without outliers

```{r}
d=C[,1:8]
pairs(d,pch=19,col=colors_Student)
```

#Principal Component Analysis

As the dimensionalities of the data grows, the feature space grws rapidly. We care about this for several reasons: first, we want to minimize computational cost. Also, when the dimensionality is big, the data starts getting more and more difficult to interpret, and it gets more difficult to detect underlying patterns.

When having so many features, we fail to see the "real" or "intrisic" dimensionality of the data. In other words, the dimensions could probably be reduced to a smaller set of dimensions.

```{r}
dim(DATOS)
head(DATOS)
```

First I define a variable Y, where my categorical variable will be saved. This is due to the fact that we need quantitative variables to perform Principal Component Analysis.
I already defined X_quan as the quantitative variables of my model.

Then, I will define de number or rows and columns as "n" and "p" respectively.

```{r}

X <- Dataset.norm[1:8]

n <- nrow(X)
n
p <- ncol(X)
p
```
We have a matrix with dimension 768 x 8.
The relationship between principal components and covariance matrix is fundamental to understand. First, the correlation matrix will help us observe if there are variables that are linearly related, and thus may be redundant when trying to explain or describe a sample.

As we can see, sometimes the size of the correlation matrix may blure our interpretations of linear correlations. 

It is still hard to trace patterns, by only basing ourselves on the correlation structure of the data. As we are not endocrinologist, who have a lot of knowledge (not even close) of the domain we are studying, we will not go forward in taking a variable out of the study by simple choice. By doing this, we may be omitting something important in our data.
This is why we will use PCA. With PCA, we will use a dimensionality reduction technique to remove the redundancy introduced by highly correlated variables.

What does PCA basically do? 
a) Removes noise from correlations
b) changes de original coordinates, and stablishes new ones (principal components)
c) Finally, helps us decide what is the real dimensionality of the data (how components are we finally considering)

Principal components are new variables that are built from linear relationships from the original variables. This new variables are uncorrelated to each other, and they try to compress the most information possible in the firsts components.

For this, we will use the previously determined normalized variables. They are also standarized, so that we won´t have any issues with scale differences.


We check, just in case, that the variables had been normalized, with a shapiro test. If the p-value of the test is smaller than 0.05, then we do not reject the null hypotesis of normality of the variable.

```{r}
shapiro.test((X$Glucose))
shapiro.test((X$Pregnancies))
shapiro.test((X$BloodPressure))
shapiro.test((X$SkinThickness))
shapiro.test((X$Age))
shapiro.test((X$Insulin))
shapiro.test((X$BMI))
shapiro.test((X$DiabetesPedigreeFunction))
```
We confirmed that they are all more or less gaussian. And by only checking the values, we can say that the variables had also been standarized, so there is no scaling problem. This is fundamental to get a valuable insight of the true dimensionality of the data, as if there variables meassured in very different scales, one will overweight the other, hiding valuable information.
```{r}
head(X)
```
We perform the PCA, and proceed to check the first 10 elements with summary.
```{r}
pca_output <- PCA(X,ncp = 8, graph = FALSE)
summary(pca_output, nbelements = 10)
```
We have as many eigenvectors and eigenvalues as variables in our numerical matrix. But the main idea of this analysis, is to be able to compress the information in less variables. It would not be of any use to take the 8 principal components (as there is no direct interpretation). In other words, if we would stick with eight variables, we would just keep on working with the original ones. 

The principal components do have a very interesting geometrical interpretation. This is because the principal components represent the direction of the data that explain a maximum amount of variance. So they act like new axes, that provide the best "angle" to analyze the data. 

Let´s try and decide how many principal components we will take.

a) Method Nº1
The screeplot is a barplot of the proportion of explained variance of each dimension.
```{r}
fviz_screeplot(pca_output,ncp=8,addlabels=T,barfill="blue",barcolor="red")
```
According to this rule, we could make use of the first 5 principal components (or only with the first two).
Checking the Kaisser-Gutman Rule, we can check which PC has eigenvalue larger than 1.
```{r}
pca_output$eig
```

According to this test, we should stick to the first three principal components, and maybe the fourth (as its eigenvalue is very close to 1).
This is because components with eigenvalue less than 1, explain less of the total variability that an original variable does, on average.
Therefore, by choosing principal components with eigenvalues greater than one, we maintain those that express more of the variability than each of the original variables.

We can also try with Parallel Analysis technique, using the package "paran".
```{r}
pca_output_ret <- paran(X,seed=1,graph = TRUE)
pca_output_ret$Retained
```

In this case, the papallel Analysis tells us that we should stay with the first three principal components, as they have an adjusted eigenvalue larger than 1. But first, we can take a look at the contribution of each variable in the construction of each principal component.
```{r}
pca_output$var$contrib
```

Get the variance of the 3 first dimensions
```{r}
pca_output$eig[,2][1:3]
```
Get the cummulative variance of the 3 first dimensions
```{r}
pca_output$eig[,3][1:3]
```

We can say that sticking with only 3  dimensions (reducing our dimensionality by more than 50%) we could explain more than 65% of the information from our dataset. 

Interpreting the contribution of each variable to the principal component can be a bit unclear by only checking the numbers. Nevertheless, we can use some graphical images to have a better understanding.
```{r}
fviz_pca_var(pca_output,col.bar="contrib",gradient.cols=c("#bb2e00","#002bbb"),repel=TRUE)
```

Creating a factor Map for variables with COS2 higher than 0.7. COS2 is also a useful measure that expresses
```{r}
fviz_pca_var(pca_output,select.var=list(cos2=0.6),repel=TRUE)
```

Checking the contribution of the most influential 5 variables on the first three principal components.
```{r}
fviz_contrib(pca_output, choice="var", axes=1,top=5)
fviz_contrib(pca_output, choice="var", axes=2,top=5)
fviz_contrib(pca_output, choice="var", axes=3,top=5)
```

Obtaining a barplot for the variables with highest COS2 in the 1st, 2nd and 3rd principal component. The squared cosine shows the importance of a component for a given observation.

```{r}
fviz_cos2(pca_output, choice = "var", axes = 1, top = 5)
fviz_cos2(pca_output, choice = "var", axes = 2, top = 5)
fviz_cos2(pca_output, choice = "var", axes = 3, top = 5)
```
We can also plot the correlation matrix of the components with the original variables. This is important, because it can show us the important variables of the original data in terms of variability.

```{r}
X_pcs <- prcomp(X,scale=TRUE)
corrplot(cor(X,X_pcs$x[,1:4]),is.corr=T)
```


By checking the principal component analysis, we can´t say that any of them really help us to understand the classification of observations between diabetic and non-diabetic. Nevertheless, it can give us a hint to what variables we can get.
As far as each component is considered:
a) Given that the variables that contribuite the most to the first principal components are all related to health, we can say that it makes a ranking in which it ranks how healthy a person is (each observation). As it is a combination of the glucose level, insuline, BMI and Skin Thickness.
b) The second principal component seems to rank observations according to some descriptive characteristics related to life cycle or maybe fertility (specially number of pregnancies, and age). It is reasonable that both variables contribute to this component as they are linearly correlated, as seen before.

We can observe the correlation between a component and a variable (thus, the information they share). This is called "loading".

```{r}
# Check if the first two components give me any sign of subgroups.
colors_X <- c(color_2,color_3)[1*(Y=="1")+1]
par(mfrow=c(1,1))
pairs(X_pcs$x[,1:3],col=colors_X,pch=19,main="The first three PCs")
```
It seems that the principal components don´t help us a lot in dividing the observations between diabetic and non-diabetic.

```{r}
summary(dat$Outcome)
fviz_pca_ind(pca_output,label="var",habillage=dat$Outcome,addEllipses=TRUE)
```

We get reassured: the first two principal components don´t seem to give us enough information to determine the subgroups: diabetic and non-diabetic. But we can still see the first principal component may be an indicator of illness (but both groups share a big surface). 

Now, let´s perform the subgroup analysis.

# PCA for both subgroups


First, I divide the sample in both subsets and normalize (and standarize) all the variables inside of each subset.

```{r,echo=FALSE}
Dataset.norm_diab <- Dataset.norm[Dataset.norm$Outcome==1,]
Dataset.norm_nodiab <- Dataset.norm[Dataset.norm$Outcome==0,]

Dataset.norm_diab[1:8] <- as.data.frame(lapply(DATOS[1:8], normalize))
X_diab <- Dataset.norm_diab[1:8] 

Dataset.norm_nodiab[1:8] <- as.data.frame(lapply(DATOS[1:8], normalize))
X_nodiab <- Dataset.norm_nodiab[1:8] 

```

## PCA for diabetic subgroup (outcome == 1)

```{r}
pca_output_diab <- PCA(X_diab,ncp = 8, graph = FALSE)
summary(pca_output_diab, nbelements = 10)
```
We now check how many components we will use for our analysis.
```{r}
# Method 1
fviz_screeplot(pca_output_diab,ncp=8,addlabels=T,barfill="blue",barcolor="red")

# Method 2
pca_output_diab$eig

# Method 3
pca_output_ret <- paran(X_diab,seed=1,graph = TRUE)
pca_output_ret$Retained
```
We also keep 3 principal components in this case.

Let´s see how much variability of the data they can explain.
```{r}
pca_output_diab$eig[,2][1:3]
```
```{r}
pca_output_diab$eig[,3][1:3]
```
We can explain almost 70% of the information of the data by only taking three variables. This is how powerful this method is: we can only make use of less than half of our variables and explain almost 70% of the variability. Of course, if we would have a larger number of variables in our original data we could see this method much more useful.

Let´s check the contributions of each variable.
```{r}
pca_output_diab$var$contrib
```

```{r}
fviz_pca_var(pca_output_diab,col.bar="contrib",gradient.cols=c("#bb2e00","#002bbb"),repel=TRUE)
```

```{r}
fviz_pca_var(pca_output_diab,select.var=list(cos2=0.6),repel=TRUE)
```
Just as before, all the original variables are positively correlated with the first principal component, but not all of them are positively correlated with the second principal component. Between the variables that have more than 0.6 as COS2, we get "age", "pregnancies" and "BMI". We can see that both age and pregnancies are negatively correlated to the second component, while the BMI is positively correlated to both first and second component.

```{r}
fviz_contrib(pca_output_diab, choice="var", axes=1,top=5)
fviz_contrib(pca_output_diab, choice="var", axes=2,top=5)
fviz_contrib(pca_output_diab, choice="var", axes=3,top=5)
```

```{r}
fviz_cos2(pca_output_diab, choice = "var", axes = 1, top = 5)
fviz_cos2(pca_output_diab, choice = "var", axes = 2, top = 5)
fviz_cos2(pca_output_diab, choice = "var", axes = 3, top = 5)
```
It seems that the first principal component encapsulates a lot of information regarding specifical issues of health. It calls our attention that the variable that contributes more to the third principal component is the Diabetes Pedigree Function. This is an important point, as the Diabetes Pedigree Function talks about the inheritence of the desease. It shows us, somehow, that diabetes may follow a genetic pattern, so those whose ancestors suffered from diabetes may get diabetes too (as now we are only studying people with diabetes). 

Now, let´s do the same with subgroup of non-diabetic.

```{r}
pca_output_no_diab <- PCA(X_nodiab,ncp = 8, graph = FALSE)
summary(pca_output_no_diab, nbelements = 10)
```
Let´s check how many principal components we are taking for our analysis.
```{r}
# Method 1
fviz_screeplot(pca_output_no_diab,ncp=8,addlabels=T,barfill="blue",barcolor="red")

# Method 2
pca_output_no_diab$eig

# Method 3
pca_output_ret <- paran(X_nodiab,seed=1,graph = TRUE)
pca_output_ret$Retained
```
And again, we will take just 3. Let´s check contributions.

```{r}
pca_output_no_diab$var$contrib
```

```{r}
fviz_pca_var(pca_output_no_diab,col.bar="contrib",gradient.cols=c("#bb2e00","#002bbb"),repel=TRUE)
```

```{r}
fviz_pca_var(pca_output_no_diab,select.var=list(cos2=0.6),repel=TRUE)
```


```{r}
fviz_contrib(pca_output_no_diab, choice="var", axes=1,top=5)
fviz_contrib(pca_output_no_diab, choice="var", axes=2,top=5)
fviz_contrib(pca_output_no_diab, choice="var", axes=3,top=5)
```
```{r}
fviz_cos2(pca_output_no_diab, choice = "var", axes = 1, top = 5)
fviz_cos2(pca_output_no_diab, choice = "var", axes = 2, top = 5)
fviz_cos2(pca_output_no_diab, choice = "var", axes = 3, top = 5)
```

Now we can see that the new variables behave similar to the ones in the whole data. The first component takes us to a ranking of health, the second one talks about cycle of life or fertility. Now the pedigree function don´t seem to have a big influence on any of the principal components.

Let´s compare the correlation matrix of both subgroups (their principal components and original variables).

```{r}
X_pc_diab <- prcomp(X_diab,scale=TRUE)
corrplot(cor(X_diab,X_pc_diab$x[,1:3]),is.corr=T)

X_pc_no_diab <- prcomp(X_nodiab, scale=TRUE)
corrplot(cor(X_nodiab,X_pc_no_diab$x[,1:3]), is.corr=T)
````
We can see that for bouth groups, both the first and second principal component look similar in regards to their relationship with the original variables. The one that changes a bit is the third one. This analysis may be telling us that our variables do not help us a lot in distinguishing diabetic from non-diabetic. But we would have to perform clustering techniques to check this more in depht.


